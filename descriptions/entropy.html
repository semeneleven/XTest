
<P align = center> <i> Кількість інформації. Ентропія. Короткі теоретичні відомості. </ I> </ p>
<P align = center> <b> Кількість інформації. </ B> </ p>
<P> & nbsp; & nbsp; & nbsp; Кількість інформації є апостеріорної характеристикою і визначає кількість інформації, яку отримують після прийому повідомлень. Якщо p (X <sub> i </ sub>) - ймовірність i-ого повідомлення, то індивідуальне кількість інформації: </ p>

<P> & nbsp; & nbsp; & nbsp; Отже, кількість інформації, що міститься в ансамблі з N повідомлень одно: </ p>
<P> <img border = 0 src = "Entrop2.gif"> </ p>
<P align = center> <u1: f eqn = "sum @ 0 1 0" /> <b> Безумовна ентропія. </ B> </ p>
<P> & nbsp; & nbsp; & nbsp; Ентропія - це середня величина невизначеності стану джерела повідомлення. Є об'єктивною характеристикою джерела повідомлень, і, якщо відома статистика повідомлень, може бути визначена апріорно, тобто до отримання повідомлень. </ P>
<P> <img border = 0 src = "Entrop3.gif"> </ p>
<P> Властивості ентропії: </ p>
<Ol start = 1 type = 1>
  <Li> Ентропія є величина речова, обмежена і невід'ємна. </ Li>
  <Li> Ентропія детермінованих повідомлень дорівнює нулю. </ Li>
  <Li> Ентропія максимальна, якщо повідомлення різновірогідні: <i> H (X) = </ i> log <i> (N). </ I> </ li>
  <Li> Ентропія системи двох альтернативних подій змінюється від 0 до 1. </ Li>
</ Ol>
<P align = center> <b> Умовна ентропія. </ B> </ p>
<P> & nbsp; & nbsp; & nbsp; Умовна ймовірність P (X <sub> i </ sub> / Y <sub> i </ sub>) показує ймовірність того, що при прийомі повідомлення Y <sub> i </ sub> було дійсно передано повідомлення X <sub> i </ sub>. Індивідуальна ентропія: <i> H (X <sub> i </ sub>) = - </ i> log <i> (P (X <sub> i </ sub>)) </ i>. Індивідуальна умовна ентропія: <i> H (X <sub> i </ sub> / Y <sub> j </ sub>) = - </ i> log <i> (P (X <sub> i </ sub > / Y <sub> j </ sub>)) </ i>. Тоді кількість інформації (повне): <i> I (X / Y) = H (X) - H (X / Y) </ i>, а це не що інше, як загальна кількість інформації з урахуванням умовної ентропії. Умовну ентропію прийнято показувати у вигляді матриці: з боку джерела і з боку приймача: </ p>
<table border=1 cellpadding=0 width=100%>
<tr>
  <td><p align=center><b>X \ Y</b></p></td>
  <td><p align=center><b>Y<sub>1</sub></b></p></td>
  <td><p align=center><b>Y<sub>2</sub></b></p></td>
  <td><p align=center><b>...</b></p></td>
  <td><p align=center><b>Y<sub>j</sub></b></p></td>
  <td><p align=center><b>...</b></p></td>
  <td><p align=center><b>Y<sub>n</sub></b></p></td>
</tr>
<tr>
  <td><p align=center><b>X<sub>1</sub></b></p></td>
  <td><p align=center>P( Y<sub>1</sub>/X<sub>1</sub> )</p></td>
  <td><p align=center>P( Y<sub>2</sub>/X<sub>1</sub> )</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>P( Y<sub>j</sub>/X<sub>1</sub> )</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>P( Y<sub>n</sub>/X<sub>1</sub> )</p></td>
</tr>
<tr>
  <td><p align=center><b>X<sub>2</sub></b></p></td>
  <td><p align=center>P( Y<sub>1</sub>/X<sub>2</sub> )</p></td>
  <td><p align=center>P( Y<sub>2</sub>/X<sub>2</sub> )</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>P( Y<sub>j</sub>/X<sub>2</sub> )</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>P( Y<sub>n</sub>/X<sub>2</sub> )</p></td>
</tr>
<tr>
  <td><p align=center><b>...</b></p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
</tr>
<tr>
  <td><p align=center><b>X<sub>i</sub></b></p></td>
  <td><p align=center>P( Y<sub>1</sub>/X<sub>i</sub> )</p></td>
  <td><p align=center>P( Y<sub>2</sub>/X<sub>i</sub> )</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>P( Y<sub>j</sub>/X<sub>i</sub> )</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>P( Y<sub>n</sub>/X<sub>i</sub> )</p></td>
</tr>
<tr>
  <td><p align=center><b>...</b></p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>...</p></td>
</tr>
<tr>
  <td><p align=center><b>X<sub>n</sub></b></p></td>
  <td><p align=center>P( Y<sub>1</sub>/X<sub>n</sub> )</p></td>
  <td><p align=center>P( Y<sub>2</sub>/X<sub>n</sub> )</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>P( Y<sub>j</sub>/X<sub>n</sub> )</p></td>
  <td><p align=center>...</p></td>
  <td><p align=center>P( Y<sub>n</sub>/X<sub>n</sub> )</p></td>
</tr>
</table>
<P> & nbsp; & nbsp; & nbsp; Для даної матриці: </ p>
<P> <img border = 0 src = "Entrop4.gif"> </ p>
<P> & nbsp; & nbsp; & nbsp; Повна умовна ентропія обчислюється за формулою: </ p>
<P> <img border = "0" src = "Entrop5.gif"> </ p>
<P> Властивості умовної ентропії: </ p>
<Ol start = 1 type = 1>
  <Li> Якщо ансамблі повідомлень Х і Y жорстко статистично пов'язані між собою, тобто при виникненні Х1 приймається Y1, при Х2 - Y2 і т.д., тоді умовна ентропія дорівнює нулю: <i> H (X / Y) = </ i> 0 і <i> H (Y / X) = </ i> 0. </ li>
  <Li> Якщо ансамбль повідомлень Х і Y є взаємно незалежними, то повна умовна ентропія Х щодо Y дорівнює безумовній ентропії Х: <i> H (X / Y) = H (X) і H (Y / X) = H (Y) </ i>. </ li>
</ Ol>
<P> & nbsp; & nbsp; & nbsp; При виконанні даної лабораторної роботи вводите результати з точністю до 0.001, тобто з трьома знаками після коми. Обчислення бажано виконувати більш точно, а потім виконувати округлення, тому що допустима похибка ± 0.001. </ p>
